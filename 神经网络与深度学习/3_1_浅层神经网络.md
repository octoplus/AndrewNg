
## 求和保持维度

keepdims选项可以保留数组的求和的维度，但会变成1维

``` python
import numpy as np
A = np.random.randn(4,3)
print A.shape
print np.sum(A, axis = 1, keepdims = True).shape
print np.sum(A, axis = 1).shape
```
结果
```
(4, 3)
(4, 1)
(4,)
```


## 反向传播的直观感受（可选）

1、bp算法中，变量梯度的形状和变量的形状一定是相同的，foo表示某个变量，dfoo表示梯度  
foo.shape == dfoo.shape

## 参数随机初始化

![p1](images/p1.png)

**权重初始化为0存在的问题**：  
同一层的神经元产生的结果完全相同，bp的时候，梯度也完全相同，梯度下降更新后，这些权重还是相同。所以结果相当于只有一个神经元。

**为什么是较小的随机数而不是较大的随机数**：  
如果使用了tanh或者sigmoid函数作为激活函数，太大的数会导致梯度消失