## 逻辑回归

$z=wx+b$

$a=\sigma(z)$

sigmoid函数：$\sigma(x)=\frac{1}{1+\exp(-x)}$



## 逻辑回归cost函数

损失函数是交叉熵

$Loss(y,\hat{y})=-y\log(\hat y)-(1-y)\log(1-\hat y)$

loss函数对应单个样本，而cost则是全体样本loss的均值

$Cost=\frac{1}{m}\sum_{i=1}^{m}Loss(y^{(i)},\hat y^{(i)})$



## 逻辑回归梯度下降

以二维的样本为例

$z=w_1x_1+w_2x_2+b$

$a=\sigma(z)$

**链式法则**：

$\frac{dL}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$

$\frac{da}{dz}=a(1-a)$  

**重要结论**：

$\frac{dL}{dz}=\frac{dL}{da}\frac{da}{dz}=a-y$

sigmoid函数求导，搭配交叉熵作为损失函数，可以使得梯度很简洁！



## m个样本上的梯度下降

求梯度

```
J=0; dw_1=0; dw_2=0; db=0
for i = 1 to m:
	z = wx+b
	a = sigma(z)
	J -= yloga + (1-y)log(1-a)
	dz = a - y
	dw_1 += x_1 * dz_1
	dw_2 += x_2 * dz_2
	db += dz
J /= m
dw_1 /= m; dw_2 /= m; db /= m
```

梯度下降

```
w_1 := w_1 - alpha * dw_1
w_2 := w_2 - alpha * dw_2
b := b - alpha * db
```









